{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import src.job_scrape_funcs as scrape\n",
    "import src.analysis as ana\n",
    "import src.models as models\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def job_pipeline(job_title, location, post_time, pages, save_path, resume_path, word_scores):\n",
    "    #TODO: try except statements\n",
    "    # Scrape jobs:\n",
    "    job_data = scrape.scrape_jobs(pages=pages, job_title=job_title, location=location, post_time=post_time)\n",
    "    date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Save:\n",
    "    index = 1\n",
    "    full_path = save_path+str(job_title).replace(\" \",\"-\")+\"_\"+str(date)+\"_\"+str(index)+\".json\"\n",
    "    while (Path.cwd()/full_path).exists():\n",
    "        index += 1\n",
    "        full_path = save_path+str(job_title).replace(\" \",\"-\")+\"_\"+str(date)+\"_\"+str(index)+\".json\"\n",
    "\n",
    "    print(\"Saving to \", full_path)\n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(job_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Find matches:\n",
    "    with open(full_path, 'r', encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    raw_df = pd.DataFrame(data[\"jobs\"])\n",
    "    df = ana.find_job_match(models.embed_model, models.nlp_model, raw_df, resume_path, models.stop_words, word_scores)\n",
    "    print(\"Generating report:\\n\\n\")\n",
    "    html = ana.generate_html_report(df)\n",
    "    soup = BeautifulSoup(html)\n",
    "    with open(\"output1.html\", \"w\") as file:\n",
    "        file.write(str(soup))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = {\n",
    "    \"phd\": 1.3,\n",
    "    \"python\": 1.1,\n",
    "    \"senior\": 0.8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 0\n",
      "10 scraped successfully.\n",
      "Scraping page 1\n",
      "Job filtered out\n",
      "19 scraped successfully.\n",
      "Scraping page 2\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "27 scraped successfully.\n",
      "Scraping page 3\n",
      "Failed to retrieve job details: 429\n",
      "37 scraped successfully.\n",
      "Scraping page 4\n",
      "47 scraped successfully.\n",
      "Scraping page 5\n",
      "57 scraped successfully.\n",
      "Scraping page 6\n",
      "Job filtered out\n",
      "66 scraped successfully.\n",
      "Scraping page 7\n",
      "76 scraped successfully.\n",
      "Scraping page 8\n",
      "86 scraped successfully.\n",
      "Scraping page 9\n",
      "96 scraped successfully.\n",
      "Scraping page 10\n",
      "Job filtered out\n",
      "105 scraped successfully.\n",
      "Scraping page 11\n",
      "115 scraped successfully.\n",
      "Scraping page 12\n",
      "Job filtered out\n",
      "124 scraped successfully.\n",
      "Scraping page 13\n",
      "Job filtered out\n",
      "132 scraped successfully.\n",
      "Scraping page 14\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "140 scraped successfully.\n",
      "Scraping page 15\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "146 scraped successfully.\n",
      "Scraping page 16\n",
      "156 scraped successfully.\n",
      "Scraping page 17\n",
      "Job filtered out\n",
      "165 scraped successfully.\n",
      "Scraping page 18\n",
      "Job filtered out\n",
      "174 scraped successfully.\n",
      "Scraping page 19\n",
      "Exception during description scrape.\n",
      "SOCKSHTTPSConnectionPool(host='www.linkedin.com', port=443): Read timed out. (read timeout=15)\n",
      "Job filtered out\n",
      "183 scraped successfully.\n",
      "Scraping page 20\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "191 scraped successfully.\n",
      "Scraping page 21\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "199 scraped successfully.\n",
      "Scraping page 22\n",
      "Job filtered out\n",
      "208 scraped successfully.\n",
      "Scraping page 23\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "214 scraped successfully.\n",
      "Scraping page 24\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "222 scraped successfully.\n",
      "Scraping page 25\n",
      "Job filtered out\n",
      "231 scraped successfully.\n",
      "Scraping page 26\n",
      "Job filtered out\n",
      "239 scraped successfully.\n",
      "Scraping page 27\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "247 scraped successfully.\n",
      "Scraping page 28\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "254 scraped successfully.\n",
      "Scraping page 29\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "261 scraped successfully.\n",
      "Scraping page 30\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "269 scraped successfully.\n",
      "Scraping page 31\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Exception during description scrape.\n",
      "SOCKSHTTPSConnectionPool(host='www.linkedin.com', port=443): Read timed out. (read timeout=15)\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "275 scraped successfully.\n",
      "Scraping page 32\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "282 scraped successfully.\n",
      "Scraping page 33\n",
      "Job filtered out\n",
      "282 scraped successfully.\n",
      "Scraping page 34\n",
      "No more jobs found.\n",
      "Exception occured during scraping.\n",
      "'NoneType' object is not iterable\n",
      "Saving to  data/job_queries/Data-Scientist_2025-04-21_1.json\n",
      "Generating report:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = job_pipeline(job_title=\"Data Scientist\", \n",
    "                  location=\"Chicago\", \n",
    "                  post_time=3, \n",
    "                  pages=40,\n",
    "                  save_path=\"data/job_queries/\", \n",
    "                  resume_path=\"data/resume.pdf\",\n",
    "                  word_scores=word_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape job listings:\n",
    "jobs = scrape.scrape_jobs(pages=2, job_title=\"Data Scientist\", location=\"Chicago\", post_time=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON:\n",
    "with open(\"data/job_queries/job_data_test.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(jobs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON:\n",
    "with open(\"data/job_queries/Data-Scientist_2025-04-14_1.json\", 'r', encoding='utf-8') as f:\n",
    "    jobs = json.load(f)\n",
    "raw_df = pd.DataFrame(jobs[\"jobs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find job matches:\n",
    "df = ana.find_job_match(models.embed_model, models.nlp_model, raw_df, \"data/resume.pdf\", models.stop_words, word_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate job match report\n",
    "html = ana.generate_html_report(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
