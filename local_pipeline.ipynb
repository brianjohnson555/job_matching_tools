{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import local.job_scrape_funcs as scrape\n",
    "import local.analysis as ana\n",
    "import local.models as models\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def job_pipeline(job_title, location, post_time, pages, save_path, resume_path, word_scores):\n",
    "    #TODO: try except statements\n",
    "    # Scrape jobs:\n",
    "    job_data = scrape.scrape_jobs(pages=pages, job_title=job_title, location=location, post_time=post_time)\n",
    "    date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Save:\n",
    "    index = 1\n",
    "    full_path = save_path+str(job_title).replace(\" \",\"-\")+\"_\"+str(date)+\"_\"+str(index)+\".json\"\n",
    "    while (Path.cwd()/full_path).exists():\n",
    "        index += 1\n",
    "        full_path = save_path+str(job_title).replace(\" \",\"-\")+\"_\"+str(date)+\"_\"+str(index)+\".json\"\n",
    "\n",
    "    print(\"Saving to \", full_path)\n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(job_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Find matches:\n",
    "    with open(full_path, 'r', encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    raw_df = pd.DataFrame(data[\"jobs\"])\n",
    "    df = ana.find_job_match(models.embed_model, models.nlp_model, raw_df, resume_path, models.stop_words, word_scores)\n",
    "    print(\"Generating report:\\n\\n\")\n",
    "    html = ana.generate_html_report(df)\n",
    "    soup = BeautifulSoup(html)\n",
    "    with open(\"output1.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(str(soup))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = {\n",
    "    \"phd\": 1.2,\n",
    "    \"python\": 1.1,\n",
    "    \"senior\": 0.8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 0\n",
      "Job filtered out\n",
      "9 scraped successfully.\n",
      "Scraping page 1\n",
      "Job filtered out\n",
      "18 scraped successfully.\n",
      "Scraping page 2\n",
      "28 scraped successfully.\n",
      "Scraping page 3\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "36 scraped successfully.\n",
      "Scraping page 4\n",
      "46 scraped successfully.\n",
      "Scraping page 5\n",
      "Job filtered out\n",
      "55 scraped successfully.\n",
      "Scraping page 6\n",
      "Job filtered out\n",
      "64 scraped successfully.\n",
      "Scraping page 7\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "72 scraped successfully.\n",
      "Scraping page 8\n",
      "Job filtered out\n",
      "81 scraped successfully.\n",
      "Scraping page 9\n",
      "Job filtered out\n",
      "90 scraped successfully.\n",
      "Scraping page 10\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "98 scraped successfully.\n",
      "Scraping page 11\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "103 scraped successfully.\n",
      "Scraping page 12\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "109 scraped successfully.\n",
      "Scraping page 13\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "112 scraped successfully.\n",
      "Scraping page 14\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "113 scraped successfully.\n",
      "Scraping page 15\n",
      "No more jobs found.\n",
      "Exception occured during scraping.\n",
      "'NoneType' object is not iterable\n",
      "Saving to  data/job_queries/machine-learning-engineer_2025-04-23_1.json\n",
      "Generating report:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = job_pipeline(job_title=\"machine learning engineer\", \n",
    "                  location=\"Chicago\", \n",
    "                  post_time=3, \n",
    "                  pages=40,\n",
    "                  save_path=\"data/job_queries/\", \n",
    "                  resume_path=\"data/resume.pdf\",\n",
    "                  word_scores=word_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape job listings:\n",
    "jobs = scrape.scrape_jobs(pages=2, job_title=\"Data Scientist\", location=\"Chicago\", post_time=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON:\n",
    "with open(\"data/job_queries/job_data_test.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(jobs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON:\n",
    "with open(\"data/job_queries/Data-Scientist_2025-04-24_1.json\", 'r', encoding='utf-8') as f:\n",
    "    jobs = json.load(f)\n",
    "raw_df = pd.DataFrame(jobs[\"jobs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist 1</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-scient...</td>\n",
       "      <td>2025-04-23T16:34:37.000Z</td>\n",
       "      <td>Develop and optimize risk strategies for PayPa...</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist, Full-time, Days</td>\n",
       "      <td>Northwestern Medicine</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>2025-04-23T20:25:38.000Z</td>\n",
       "      <td>Enabling digital solutions by leveraging capab...</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Machine Learning Engineer, Full-time, Days</td>\n",
       "      <td>Northwestern Medicine</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mach...</td>\n",
       "      <td>2025-04-23T20:25:35.000Z</td>\n",
       "      <td>Enabling digital solutions by leveraging capab...</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>Lessen</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/machine-lea...</td>\n",
       "      <td>2025-04-23T17:16:49.000Z</td>\n",
       "      <td>Design, deploy, fine-tune, and evaluate large ...</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>Evolve Group</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/machine-lea...</td>\n",
       "      <td>2025-04-24T10:21:46.000Z</td>\n",
       "      <td>Machine Learning Engineer â€“ Quantitative Hedge...</td>\n",
       "      <td>Greater Chicago Area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                company  \\\n",
       "0                                   Data Scientist 1                 PayPal   \n",
       "1             Senior Data Scientist, Full-time, Days  Northwestern Medicine   \n",
       "2  Senior Machine Learning Engineer, Full-time, Days  Northwestern Medicine   \n",
       "3                          Machine Learning Engineer                 Lessen   \n",
       "4                          Machine Learning Engineer           Evolve Group   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.linkedin.com/jobs/view/data-scient...   \n",
       "1  https://www.linkedin.com/jobs/view/senior-data...   \n",
       "2  https://www.linkedin.com/jobs/view/senior-mach...   \n",
       "3  https://www.linkedin.com/jobs/view/machine-lea...   \n",
       "4  https://www.linkedin.com/jobs/view/machine-lea...   \n",
       "\n",
       "                       date  \\\n",
       "0  2025-04-23T16:34:37.000Z   \n",
       "1  2025-04-23T20:25:38.000Z   \n",
       "2  2025-04-23T20:25:35.000Z   \n",
       "3  2025-04-23T17:16:49.000Z   \n",
       "4  2025-04-24T10:21:46.000Z   \n",
       "\n",
       "                                         description              location  \n",
       "0  Develop and optimize risk strategies for PayPa...           Chicago, IL  \n",
       "1  Enabling digital solutions by leveraging capab...           Chicago, IL  \n",
       "2  Enabling digital solutions by leveraging capab...           Chicago, IL  \n",
       "3  Design, deploy, fine-tune, and evaluate large ...           Chicago, IL  \n",
       "4  Machine Learning Engineer â€“ Quantitative Hedge...  Greater Chicago Area  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find job matches:\n",
    "df = ana.find_job_match(models.embed_model, models.nlp_model, raw_df, \"docker_analyzer/data/resume.pdf\", models.stop_words, word_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate job match report\n",
    "html = ana.generate_html_report(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "# Load JSON:\n",
    "with open(\"data/postman_outputs/response_5_5\", 'r', encoding='utf-8') as f:\n",
    "    html = json.load(f)['output']['Payload']['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "with open(\"output1.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(str(soup))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
