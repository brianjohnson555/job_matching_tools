{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import local.job_scrape_funcs as scrape\n",
    "import local.analysis as ana\n",
    "import local.models as models\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def job_pipeline(job_title, location, post_time, pages, save_path, resume_path, word_scores):\n",
    "    #TODO: try except statements\n",
    "    # Scrape jobs:\n",
    "    job_data = scrape.scrape_jobs(pages=pages, job_title=job_title, location=location, post_time=post_time)\n",
    "    date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Save:\n",
    "    index = 1\n",
    "    full_path = save_path+str(job_title).replace(\" \",\"-\")+\"_\"+str(date)+\"_\"+str(index)+\".json\"\n",
    "    while (Path.cwd()/full_path).exists():\n",
    "        index += 1\n",
    "        full_path = save_path+str(job_title).replace(\" \",\"-\")+\"_\"+str(date)+\"_\"+str(index)+\".json\"\n",
    "\n",
    "    print(\"Saving to \", full_path)\n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(job_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Find matches:\n",
    "    with open(full_path, 'r', encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    raw_df = pd.DataFrame(data[\"jobs\"])\n",
    "    df = ana.find_job_match(models.embed_model, models.nlp_model, raw_df, resume_path, models.stop_words, word_scores)\n",
    "    print(\"Generating report:\\n\\n\")\n",
    "    html = ana.generate_html_report(df)\n",
    "    soup = BeautifulSoup(html)\n",
    "    with open(\"output1.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(str(soup))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = {\n",
    "    \"phd\": 1.2,\n",
    "    \"python\": 1.1,\n",
    "    \"senior\": 0.8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 0\n",
      "Job filtered out\n",
      "9 scraped successfully.\n",
      "Scraping page 1\n",
      "Job filtered out\n",
      "18 scraped successfully.\n",
      "Scraping page 2\n",
      "28 scraped successfully.\n",
      "Scraping page 3\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "36 scraped successfully.\n",
      "Scraping page 4\n",
      "46 scraped successfully.\n",
      "Scraping page 5\n",
      "Job filtered out\n",
      "55 scraped successfully.\n",
      "Scraping page 6\n",
      "Job filtered out\n",
      "64 scraped successfully.\n",
      "Scraping page 7\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "72 scraped successfully.\n",
      "Scraping page 8\n",
      "Job filtered out\n",
      "81 scraped successfully.\n",
      "Scraping page 9\n",
      "Job filtered out\n",
      "90 scraped successfully.\n",
      "Scraping page 10\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "98 scraped successfully.\n",
      "Scraping page 11\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "103 scraped successfully.\n",
      "Scraping page 12\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "109 scraped successfully.\n",
      "Scraping page 13\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "112 scraped successfully.\n",
      "Scraping page 14\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "Job filtered out\n",
      "113 scraped successfully.\n",
      "Scraping page 15\n",
      "No more jobs found.\n",
      "Exception occured during scraping.\n",
      "'NoneType' object is not iterable\n",
      "Saving to  data/job_queries/machine-learning-engineer_2025-04-23_1.json\n",
      "Generating report:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = job_pipeline(job_title=\"machine learning engineer\", \n",
    "                  location=\"Chicago\", \n",
    "                  post_time=3, \n",
    "                  pages=40,\n",
    "                  save_path=\"data/job_queries/\", \n",
    "                  resume_path=\"data/resume.pdf\",\n",
    "                  word_scores=word_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape job listings:\n",
    "jobs = scrape.scrape_jobs(pages=2, job_title=\"Data Scientist\", location=\"Chicago\", post_time=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON:\n",
    "with open(\"data/job_queries/job_data_test.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(jobs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON:\n",
    "with open(\"data/job_queries/Data-Scientist_2025-04-23_1.json\", 'r', encoding='utf-8') as f:\n",
    "    jobs = json.load(f)\n",
    "raw_df = pd.DataFrame(jobs[\"jobs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find job matches:\n",
    "df = ana.find_job_match(models.embed_model, models.nlp_model, raw_df, \"data/resume.pdf\", models.stop_words, word_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate job match report\n",
    "html = ana.generate_html_report(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "with open(\"output1.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(str(soup))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
